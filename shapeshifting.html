<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<!-- CSS -->
<title>src/arraymancer/tensor/shapeshifting</title>
<link rel="stylesheet" type="text/css" href="nimdoc.out.css">

<script type="text/javascript" src="dochack.js"></script>

<script type="text/javascript">
function main() {
  var pragmaDots = document.getElementsByClassName("pragmadots");
  for (var i = 0; i < pragmaDots.length; i++) {
    pragmaDots[i].onclick = function(event) {
      // Hide tease
      event.target.parentNode.style.display = "none";
      // Show actual
      event.target.parentNode.nextElementSibling.style.display = "inline";
    }
  }

  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  function switchTheme(e) {
      if (e.target.checked) {
          document.documentElement.setAttribute('data-theme', 'dark');
          localStorage.setItem('theme', 'dark');
      } else {
          document.documentElement.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
      }
  }

  toggleSwitch.addEventListener('change', switchTheme, false);


  if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    document.documentElement.setAttribute('data-theme', "dark");
    toggleSwitch.checked = true;
  } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', "light");
    toggleSwitch.checked = false;
  } else {
    const currentTheme = localStorage.getItem('theme') ? localStorage.getItem('theme') : null;
    if (currentTheme) {
      document.documentElement.setAttribute('data-theme', currentTheme);

      if (currentTheme === 'dark') {
        toggleSwitch.checked = true;
      }
    }
  }
}
</script>

</head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Arraymancer - src/arraymancer/tensor/shapeshifting</title>

<link href="docutils.css" rel="stylesheet" type="text/css"/>
<link href="nav.css" rel="stylesheet" type="text/css"/>

<link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_gray_6d6d6d.png?resize=150%2C150" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>

<body onload="main()">
<div class="document" id="documentId">
  <div class="container">
    <h1 class="title">src/arraymancer/tensor/shapeshifting</h1>
    <div class="row">
  <div class="three columns">
    <div class="theme-select-wrapper">
      <label for="theme-select">Theme:&nbsp;</label>
      <select id="theme-select" onchange="setTheme(this.value)">
        <option value="auto">ðŸŒ— Match OS</option>
        <option value="dark">ðŸŒ‘ Dark</option>
        <option value="light">ðŸŒ• Light</option>
      </select>
    </div>
    <div id="global-links">
      <ul class="simple">
        <li><a id="indexLink" href="theindex.html">Index</a></li>
      </ul>
    </div>
    <div id="searchInputDiv">
      Search: <input type="search" id="searchInput" onkeyup="search()"/>
    </div>
    <div>
      Group by:
      <select onchange="groupBy(this.value)">
        <option value="section">Section</option>
        <option value="type">Type</option>
      </select>
    </div>
    <ul class="simple simple-toc" id="toc-list">
  <li><a class="reference" id="Z1-2-3-4_toc" href="#Z1-2-3-4">1     2     3     4</a></li>
<li><a class="reference" id="Z1-2-3-4-5-6-7_toc" href="#Z1-2-3-4-5-6-7">1     2     3     4     5     6     7</a></li>
<li><a class="reference" id="Z1-2-3-4-5-6-7_toc" href="#Z1-2-3-4-5-6-7">1     2     3     4     5     6     7</a></li>
<li><a class="reference" id="on-rankminus1-tensors-but-first-input-tensor-has-rank-2-assertiondefectassertiondefect_toc" href="#on-rankminus1-tensors-but-first-input-tensor-has-rank-2-assertiondefectassertiondefect">on rank-1 tensors but first input tensor has rank 2 <a class="reference internal" href="#AssertionDefect">AssertionDefect</a></a></li>
<li>
  <a class="reference reference-toplevel" href="#6" id="56">Imports</a>
</li>
<li>
  <details open>
    <summary><a class="reference reference-toplevel" href="#12" id="62">Procs</a></summary>
    <ul class="simple simple-toc-section">
      <ul class="simple nested-toc-section">append
  <li><a class="reference" href="#append%2CTensor%5BT%5D%2CTensor%5BT%5D" title="append[T](t: Tensor[T]; values: Tensor[T]): Tensor[T]">append[T](t: Tensor[T]; values: Tensor[T]): Tensor[T]</a></li>
<li><a class="reference" href="#append%2CTensor%5BT%5D%2Cvarargs%5BT%5D" title="append[T](t: Tensor[T]; values: varargs[T]): Tensor[T]">append[T](t: Tensor[T]; values: varargs[T]): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">asContiguous
  <li><a class="reference" href="#asContiguous%2CTensor%5BT%5D%2COrderType%2Cbool" title="asContiguous[T](t: Tensor[T]; layout: OrderType = rowMajor; force: bool = false): Tensor[
    T]">asContiguous[T](t: Tensor[T]; layout: OrderType = rowMajor; force: bool = false): Tensor[
    T]</a></li>

</ul>
<ul class="simple nested-toc-section">broadcast
  <li><a class="reference" href="#broadcast%2CT%2CMetadata" title="broadcast[T: SomeNumber](val: T; shape: Metadata): Tensor[T]">broadcast[T: SomeNumber](val: T; shape: Metadata): Tensor[T]</a></li>
<li><a class="reference" href="#broadcast%2CT%2Cvarargs%5Bint%5D" title="broadcast[T: SomeNumber](val: T; shape: varargs[int]): Tensor[T]">broadcast[T: SomeNumber](val: T; shape: varargs[int]): Tensor[T]</a></li>
<li><a class="reference" href="#broadcast%2CTensor%5BT%5D%2CMetadata" title="broadcast[T](t: Tensor[T]; shape: Metadata): Tensor[T]">broadcast[T](t: Tensor[T]; shape: Metadata): Tensor[T]</a></li>
<li><a class="reference" href="#broadcast%2CTensor%5BT%5D%2Cvarargs%5Bint%5D" title="broadcast[T](t: Tensor[T]; shape: varargs[int]): Tensor[T]">broadcast[T](t: Tensor[T]; shape: varargs[int]): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">broadcast2
  <li><a class="reference" href="#broadcast2%2CTensor%5BT%5D%2CTensor%5BT%5D" title="broadcast2[T](a, b: Tensor[T]): tuple[a, b: Tensor[T]]">broadcast2[T](a, b: Tensor[T]): tuple[a, b: Tensor[T]]</a></li>

</ul>
<ul class="simple nested-toc-section">chunk
  <li><a class="reference" href="#chunk%2CTensor%5BT%5D%2CPositive%2CNatural" title="chunk[T](t: Tensor[T]; nb_chunks: Positive; axis: Natural): seq[Tensor[T]]">chunk[T](t: Tensor[T]; nb_chunks: Positive; axis: Natural): seq[Tensor[T]]</a></li>

</ul>
<ul class="simple nested-toc-section">concat
  <li><a class="reference" href="#concat%2Cvarargs%5BTensor%5BT%5D%5D%2Cint" title="concat[T](t_list: varargs[Tensor[T]]; axis: int): Tensor[T]">concat[T](t_list: varargs[Tensor[T]]; axis: int): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">flatten
  <li><a class="reference" href="#flatten%2CTensor" title="flatten(t: Tensor): Tensor">flatten(t: Tensor): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">moveaxis
  <li><a class="reference" href="#moveaxis%2CTensor%2CNatural%2CNatural" title="moveaxis(t: Tensor; initial: Natural; target: Natural): Tensor">moveaxis(t: Tensor; initial: Natural; target: Natural): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">permute
  <li><a class="reference" href="#permute%2CTensor%2Cvarargs%5Bint%5D" title="permute(t: Tensor; dims: varargs[int]): Tensor">permute(t: Tensor; dims: varargs[int]): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">repeat_values
  <li><a class="reference" href="#repeat_values%2CTensor%5BT%5D%2Cint%2Cint" title="repeat_values[T](t: Tensor[T]; reps: int; axis = -1): Tensor[T]">repeat_values[T](t: Tensor[T]; reps: int; axis = -1): Tensor[T]</a></li>
<li><a class="reference" href="#repeat_values%2CTensor%5BT%5D%2CopenArray%5Bint%5D" title="repeat_values[T](t: Tensor[T]; reps: openArray[int]): Tensor[T]">repeat_values[T](t: Tensor[T]; reps: openArray[int]): Tensor[T]</a></li>
<li><a class="reference" href="#repeat_values%2CTensor%5BT%5D%2CTensor%5Bint%5D" title="repeat_values[T](t: Tensor[T]; reps: Tensor[int]): Tensor[T]">repeat_values[T](t: Tensor[T]; reps: Tensor[int]): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">reshape
  <li><a class="reference" href="#reshape%2CTensor%2CMetadata" title="reshape(t: Tensor; new_shape: Metadata): Tensor">reshape(t: Tensor; new_shape: Metadata): Tensor</a></li>
<li><a class="reference" href="#reshape%2CTensor%2Cvarargs%5Bint%5D" title="reshape(t: Tensor; new_shape: varargs[int]): Tensor">reshape(t: Tensor; new_shape: varargs[int]): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">reshape_infer
  <li><a class="reference" href="#reshape_infer%2CTensor%2Cvarargs%5Bint%5D" title="reshape_infer(t: Tensor; new_shape: varargs[int]): Tensor">reshape_infer(t: Tensor; new_shape: varargs[int]): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">roll
  <li><a class="reference" href="#roll%2CTensor%5BT%5D%2Cint" title="roll[T](t: Tensor[T]; shift: int): Tensor[T]">roll[T](t: Tensor[T]; shift: int): Tensor[T]</a></li>
<li><a class="reference" href="#roll%2CTensor%5BT%5D%2Cint%2CNatural" title="roll[T](t: Tensor[T]; shift: int; axis: Natural): Tensor[T]">roll[T](t: Tensor[T]; shift: int; axis: Natural): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">split
  <li><a class="reference" href="#split%2CTensor%5BT%5D%2CPositive%2CNatural" title="split[T](t: Tensor[T]; chunk_size: Positive; axis: Natural): seq[Tensor[T]]">split[T](t: Tensor[T]; chunk_size: Positive; axis: Natural): seq[Tensor[T]]</a></li>

</ul>
<ul class="simple nested-toc-section">squeeze
  <li><a class="reference" href="#squeeze%2CAnyTensor" title="squeeze(t: AnyTensor): AnyTensor">squeeze(t: AnyTensor): AnyTensor</a></li>
<li><a class="reference" href="#squeeze%2CTensor%2CNatural" title="squeeze(t: Tensor; axis: Natural): Tensor">squeeze(t: Tensor; axis: Natural): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">stack
  <li><a class="reference" href="#stack%2Cvarargs%5BTensor%5BT%5D%5D%2CNatural" title="stack[T](tensors: varargs[Tensor[T]]; axis: Natural = 0): Tensor[T]">stack[T](tensors: varargs[Tensor[T]]; axis: Natural = 0): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">tile
  <li><a class="reference" href="#tile%2CTensor%5BT%5D%2Cvarargs%5Bint%5D" title="tile[T](t: Tensor[T]; reps: varargs[int]): Tensor[T]">tile[T](t: Tensor[T]; reps: varargs[int]): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">transpose
  <li><a class="reference" href="#transpose%2CTensor" title="transpose(t: Tensor): Tensor">transpose(t: Tensor): Tensor</a></li>

</ul>
<ul class="simple nested-toc-section">unsqueeze
  <li><a class="reference" href="#unsqueeze%2CTensor%2CNatural" title="unsqueeze(t: Tensor; axis: Natural): Tensor">unsqueeze(t: Tensor; axis: Natural): Tensor</a></li>

</ul>

    </ul>
  </details>
</li>
<li>
  <details open>
    <summary><a class="reference reference-toplevel" href="#18" id="68">Templates</a></summary>
    <ul class="simple simple-toc-section">
      <ul class="simple nested-toc-section">bc
  <li><a class="reference" href="#bc.t%2C%2CMetadata" title="bc(t: (Tensor | SomeNumber); shape: Metadata): untyped">bc(t: (Tensor | SomeNumber); shape: Metadata): untyped</a></li>
<li><a class="reference" href="#bc.t%2C%2Cvarargs%5Bint%5D" title="bc(t: (Tensor | SomeNumber); shape: varargs[int]): untyped">bc(t: (Tensor | SomeNumber); shape: varargs[int]): untyped</a></li>

</ul>

    </ul>
  </details>
</li>

</ul>

  </div>
  <div class="nine columns" id="content">
    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L1"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L1" class="link-seesrc" target="_blank" >Edit</a>

    <div id="tocRoot"></div>
    
    <p class="module-desc"></p>
    <div class="section" id="6">
  <h1><a class="toc-backref" href="#6">Imports</a></h1>
  <dl class="item">
    <a class="reference external" href="p_shapeshifting.html">p_shapeshifting</a>, <a class="reference external" href="p_checks.html">p_checks</a>, <a class="reference external" href="p_accessors_macros_write.html">p_accessors_macros_write</a>, <a class="reference external" href="p_empty_tensors.html">p_empty_tensors</a>, <a class="reference external" href="accessors.html">accessors</a>, <a class="reference external" href="data_structure.html">data_structure</a>, <a class="reference external" href="init_cpu.html">init_cpu</a>, <a class="reference external" href="higher_order_applymap.html">higher_order_applymap</a>
  </dl>
</div>
<div class="section" id="12">
  <h1><a class="toc-backref" href="#12">Procs</a></h1>
  <dl class="item">
    <div id="append-procs-all">
  <div id="append,Tensor[T],Tensor[T]">
  <dt><pre><span class="Keyword">proc</span> <a href="#append%2CTensor%5BT%5D%2CTensor%5BT%5D"><span class="Identifier">append</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">values</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Create a copy of an rank-1 input tensor with values appended to its end</p>
<p>Inputs:</p>
<ul class="simple"><li>Rank-1 tensor</li>
<li>Rank-1 tensor of extra values to append</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A copy of the input tensor t with the extra values appended at the end.</li>
</ul>
<p>Notes: Append does not occur in-place (a new tensor is allocated and filled). To concatenate more than one tensor or tensors that you must use <tt class="docutils literal"><span class="pre"><span class="Identifier">concat</span></span></tt>. Compared to numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">append</span></span></tt>, this proc requires that you explicitly flatten the inputs if they are not rank-1 tensors. It also does not support the <tt class="docutils literal"><span class="pre"><span class="Identifier">axis</span></span></tt> parameter. If you want to append the values along a specific axis, you should use <tt class="docutils literal"><span class="pre"><span class="Identifier">concat</span></span></tt> instead. Examples:<blockquote class="markdown-quote"><p><p>echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, <a class="reference internal" href="#4, 5, 6, 7">4, 5, 6, 7</a>.toTensor) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#9">9</a>&quot; on backend &quot;Cpu&quot; #    1     2     3     4     5     6     7</p>
<p>echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, [<a class="reference internal" href="#4, 5, 6">4, 5, 6</a>, <a class="reference internal" href="#7, 8, 9">7, 8, 9</a>].toTensor) # Error: unhandled exception: <tt class="docutils literal"><span class="pre"><span class="Identifier">values</span><span class="Operator">.</span><span class="Identifier">rank</span> <span class="Operator">==</span> <span class="DecNumber">1</span></span></tt> append only works # on rank-1 tensors but extra values tensor has rank 2 <a class="reference internal" href="#AssertionDefect">AssertionDefect</a></p>
<p>echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, [<a class="reference internal" href="#4, 5, 6">4, 5, 6</a>, <a class="reference internal" href="#7, 8, 9">7, 8, 9</a>].toTensor.flatten) #    1     2     3     4     5     6     7     8     9</p>
</p></blockquote>
</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L295"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L295" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="append,Tensor[T],varargs[T]">
  <dt><pre><span class="Keyword">proc</span> <a href="#append%2CTensor%5BT%5D%2Cvarargs%5BT%5D"><span class="Identifier">append</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">values</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Create a copy of an rank-1 input tensor with one or more values appended to its end</p>
<p>Inputs:</p>
<ul class="simple"><li>Rank-1 tensor of type T</li>
<li>An open array or a list of values of type T</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A copy of the input tensor t with the extra values appended at the end.</li>
</ul>
<p>Notes: Append does not occur in-place (a new tensor is allocated and filled). Compared to numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">append</span></span></tt>, this proc requires that you explicitly flatten the input tensor if its rank is greater than 1. It also does not support the <tt class="docutils literal"><span class="pre"><span class="Identifier">axis</span></span></tt> parameter. If you want to append values along a specific axis, you should use <tt class="docutils literal"><span class="pre"><span class="Identifier">concat</span></span></tt> instead. Examples:</p>
<ul class="simple"><li>Append a single value</li>
</ul>
<blockquote class="markdown-quote"><p>echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, 4) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#9">9</a>&quot; on backend &quot;Cpu&quot;</p><p>
<h1><a class="toc-backref" id="Z1-2-3-4" href="#Z1-2-3-4">1     2     3     4</a></h1><ul class="simple"><li>Append a multiple values echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, <a class="reference internal" href="#4, 5, 6, 7">4, 5, 6, 7</a>) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#9">9</a>&quot; on backend &quot;Cpu&quot;</li>
</ul>
</p><p>
<h1><a class="toc-backref" id="Z1-2-3-4-5-6-7" href="#Z1-2-3-4-5-6-7">1     2     3     4     5     6     7</a></h1><ul class="simple"><li>Append an openArray of values echo append(<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>.toTensor, <a class="reference internal" href="#4, 5, 6, 7">4, 5, 6, 7</a>) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#9">9</a>&quot; on backend &quot;Cpu&quot;</li>
</ul>
</p><p>
<h1><a class="toc-backref" id="Z1-2-3-4-5-6-7" href="#Z1-2-3-4-5-6-7">1     2     3     4     5     6     7</a></h1><ul class="simple"><li>Only rank-1 tensors are supported echo append([<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>, <a class="reference internal" href="#4, 5, 6">4, 5, 6</a>].toTensor, <a class="reference internal" href="#7, 8, 9">7, 8, 9</a>) # Error: unhandled exception: <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">rank</span> <span class="Operator">==</span> <span class="DecNumber">1</span></span></tt> append only works</li>
</ul>
</p><p>
<h1><a class="toc-backref" id="on-rankminus1-tensors-but-first-input-tensor-has-rank-2-assertiondefectassertiondefect" href="#on-rankminus1-tensors-but-first-input-tensor-has-rank-2-assertiondefectassertiondefect">on rank-1 tensors but first input tensor has rank 2 <a class="reference internal" href="#AssertionDefect">AssertionDefect</a></a></h1><ul class="simple"><li>Flatten higher ranked tensors before appending echo append([<a class="reference internal" href="#1, 2, 3">1, 2, 3</a>, <a class="reference internal" href="#4, 5, 6">4, 5, 6</a>].toTensor.flatten, <a class="reference internal" href="#7, 8, 9">7, 8, 9</a>) #    1     2     3     4     5     6     7     8     9</li>
</ul>
</p></blockquote>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L332"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L332" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="asContiguous-procs-all">
  <div id="asContiguous,Tensor[T],OrderType,bool">
  <dt><pre><span class="Keyword">proc</span> <a href="#asContiguous%2CTensor%5BT%5D%2COrderType%2Cbool"><span class="Identifier">asContiguous</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">layout</span><span class="Other">:</span> <span class="Identifier">OrderType</span> <span class="Other">=</span> <span class="Identifier">rowMajor</span><span class="Other">;</span>
                     <span class="Identifier">force</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Transform a tensor with general striding to a Tensor with contiguous layout.</p>
<p>By default tensor will be rowMajor.</p>
<p>The layout is kept if the tensor is already contiguous (C Major or F major) The &quot;force&quot; parameter can force re-ordering to a specific layout.</p>
<p>Result is always a fully packed tensor even if the input is a contiguous slice.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L38"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L38" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="broadcast-procs-all">
  <div id="broadcast,T,Metadata">
  <dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CT%2CMetadata"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">:</span> <span class="Identifier">SomeNumber</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">val</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="datatypes.html#Metadata"><span class="Identifier">Metadata</span></a><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span><span class="Other">,</span>
    <span class="Identifier">noSideEffect</span>.}</pre></dt>
  <dd>
    
    <p>Broadcast a number</p>
<p>Input:</p>
<ul class="simple"><li>a number to be broadcasted</li>
<li>a tensor shape that will be broadcasted to</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with the broadcasted shape where all elements has the broadcasted value</li>
</ul>
<p>The broadcasting is made using tensor data of size 1 and 0 strides, i.e. the operation is memory efficient.</p>
<p>Warning âš : A broadcasted tensor should not be modified and only used for computation. Modifying any value from this broadcasted tensor will change all its values.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L157"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L157" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="broadcast,T,varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CT%2Cvarargs%5Bint%5D"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">:</span> <span class="Identifier">SomeNumber</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">val</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Broadcast a number</p>
<p>Input:</p>
<ul class="simple"><li>a number to be broadcasted</li>
<li>a tensor shape that will be broadcasted to</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with the broadcasted shape where all elements has the broadcasted value</li>
</ul>
<p>The broadcasting is made using tensor data of size 1 and 0 strides, i.e. the operation is memory efficient.</p>
<p>Warning âš : A broadcasted tensor should not be modified and only used for computation. Modifying any value from this broadcasted tensor will change all its values.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L136"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L136" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="broadcast,Tensor[T],Metadata">
  <dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CTensor%5BT%5D%2CMetadata"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="datatypes.html#Metadata"><span class="Identifier">Metadata</span></a><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span><span class="Other">,</span>
    <span class="Identifier">noSideEffect</span>.}</pre></dt>
  <dd>
    
    <p>Explicitly broadcast a tensor to the specified shape.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<p>Warning âš : A broadcasted tensor should not be modified and only used for computation.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L124"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L124" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="broadcast,Tensor[T],varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CTensor%5BT%5D%2Cvarargs%5Bint%5D"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span><span class="Other">,</span>
    <span class="Identifier">noSideEffect</span>.}</pre></dt>
  <dd>
    
    <p>Explicitly broadcast a tensor to the specified shape.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<p>Warning âš : A broadcasted tensor should not be modified and only used for computation.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L112"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L112" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="broadcast2-procs-all">
  <div id="broadcast2,Tensor[T],Tensor[T]">
  <dt><pre><span class="Keyword">proc</span> <a href="#broadcast2%2CTensor%5BT%5D%2CTensor%5BT%5D"><span class="Identifier">broadcast2</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">a</span><span class="Other">,</span> <span class="Identifier">b</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Keyword">tuple</span><span class="Other">[</span><span class="Identifier">a</span><span class="Other">,</span> <span class="Identifier">b</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> {.<span class="Identifier">noSideEffect</span><span class="Other">,</span>
    <span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Broadcast 2 tensors so they have compatible shapes for element-wise computations.</p>
<p>Tensors in the tuple can be accessed with output.a and output.b</p>
<p>The returned broadcasted Tensors share the underlying data with the input.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<p>Warning âš : This is a no-copy operation, data is shared with the input. This proc does not guarantee that a <tt class="docutils literal"><span class="pre">let</span></tt> value is immutable. A broadcasted tensor should not be modified and only used for computation.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L186"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L186" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="chunk-procs-all">
  <div id="chunk,Tensor[T],Positive,Natural">
  <dt><pre><span class="Keyword">func</span> <a href="#chunk%2CTensor%5BT%5D%2CPositive%2CNatural"><span class="Identifier">chunk</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">nb_chunks</span><span class="Other">:</span> <span class="Identifier">Positive</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> {.
    <span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Splits a Tensor into n chunks along the specified axis.</p>
<p>In case a tensor cannot be split evenly, with la == length_axis, n = n_chunks it returns la mod n subtensors of size <tt class="docutils literal"><span class="pre"><span class="Punctuation">(</span><span class="Identifier">la</span> <span class="Keyword">div</span> <span class="Identifier">n</span><span class="Punctuation">)</span> <span class="Operator">+</span> <span class="DecNumber">1</span></span></tt> the rest of size <tt class="docutils literal"><span class="pre"><span class="Identifier">la</span> <span class="Keyword">div</span> <span class="Identifier">n</span></span></tt>.</p>
<p>This is consistent with numpy array_split</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L431"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L431" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="concat-procs-all">
  <div id="concat,varargs[Tensor[T]],int">
  <dt><pre><span class="Keyword">proc</span> <a href="#concat%2Cvarargs%5BTensor%5BT%5D%5D%2Cint"><span class="Identifier">concat</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t_list</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Concatenate tensors Input:<ul class="simple"><li>Tensors</li>
<li>An axis (dimension)</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L259"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L259" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="flatten-procs-all">
  <div id="flatten,Tensor">
  <dt><pre><span class="Keyword">proc</span> <a href="#flatten%2CTensor"><span class="Identifier">flatten</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span><span class="Other">,</span> <span class="Identifier">inline</span>.}</pre></dt>
  <dd>
    
    <p>Flatten a tensor, returning a rank-1 tensor with the same data as the input.</p>
<p>This is the same as <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">reshape</span><span class="Punctuation">(</span><span class="Punctuation">[</span><span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">size</span><span class="Operator">.</span><span class="Identifier">int</span><span class="Punctuation">]</span><span class="Punctuation">)</span></span></tt>. Therefore, if possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.</p>
<p>Input:</p>
<ul class="simple"><li>a tensor</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor rank-1 tensor with the same data as the input.</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L99"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L99" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="moveaxis-procs-all">
  <div id="moveaxis,Tensor,Natural,Natural">
  <dt><pre><span class="Keyword">proc</span> <a href="#moveaxis%2CTensor%2CNatural%2CNatural"><span class="Identifier">moveaxis</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">initial</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">;</span> <span class="Identifier">target</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Move one of the axes of a tensor into a new position Input:<ul class="simple"><li>a tensor</li>
<li>the initial position of the axes to move</li>
<li>the target position of the axes to move</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with moved axes but sharing the same data</li>
</ul>
<p>See also:</p>
<ul class="simple"><li>permute</li>
</ul>
<p>Usage: .. code:: nim # move dim 0 to position 2, which makes # dim 1 become dim 0 and dim 2 become dim 1 a.moveaxis(0, 2) Notes: Call <tt class="docutils literal"><span class="pre"><span class="Operator">.</span><span class="Identifier">clone</span><span class="Punctuation">(</span><span class="Punctuation">)</span></span></tt> if you want to make a copy of the data, otherwise changes to the data of returned tensor will affect the input tensor.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L228"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L228" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="permute-procs-all">
  <div id="permute,Tensor,varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#permute%2CTensor%2Cvarargs%5Bint%5D"><span class="Identifier">permute</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">dims</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span>.}</pre></dt>
  <dd>
    
    Permute the dimensions of a tensor into a different order Input:<ul class="simple"><li>a tensor</li>
<li>the new dimension order</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with re-ordered dimensions but sharing the same data</li>
</ul>
<p>See also:</p>
<ul class="simple"><li>moveaxis</li>
</ul>
<p>Usage: .. code:: nim # keep dim 0 at position 0 and swap dims 1 and 2 a.permute(0,2,1) Notes: Call <tt class="docutils literal"><span class="pre"><span class="Operator">.</span><span class="Identifier">clone</span><span class="Punctuation">(</span><span class="Punctuation">)</span></span></tt> if you want to make a copy of the data, otherwise changes to the data of returned tensor will affect the input tensor.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L207"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L207" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="repeat_values-procs-all">
  <div id="repeat_values,Tensor[T],int,int">
  <dt><pre><span class="Keyword">proc</span> <a href="#repeat_values%2CTensor%5BT%5D%2Cint%2Cint"><span class="Identifier">repeat_values</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">reps</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">;</span> <span class="Identifier">axis</span> <span class="Other">=</span> <span class="DecNumber">-1</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Create a new tensor with each value repeated (the same amount of) <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> times</p>
<p>Inputs:</p>
<ul class="simple"><li>t: A tensor.</li>
<li>reps: The integer number of times that each value must be repeated.</li>
<li>axis: The axis over which values will be repeated. Defaults to the last axis.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A new tensor containing the values of the input tensor repeated <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> times over the selected axis.</li>
</ul>
<p>Notes:</p>
<ul class="simple"><li>All values are repeated (the same amount of) <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> times along the selected axis. This makes the output shape the same as the input shape except at the selected axis, which is <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> times greater.</li>
<li>There are an alternative versions of this function which take a list of <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> instead of a single <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> value.</li>
<li>The equivalent numpy function is called <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat</span></span></tt>, while the equivalent Matlab function is called <tt class="docutils literal"><span class="pre"><span class="Identifier">repelem</span></span></tt>. Different names where chosen here to avoid confusion with nim's <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat</span></span></tt> function which behaves like numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">tile</span></span></tt>, not like this function.</li>
</ul>
<p>Examples:</p>
<p><pre class="listing">
<span class="Keyword">let</span> <span class="Identifier">t</span> <span class="Operator">=</span> <span class="Identifier">arange</span><span class="Punctuation">(</span><span class="DecNumber">6</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">reshape</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="DecNumber">3</span><span class="Punctuation">)</span>
<span class="Identifier">echo</span> <span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">repeat_values</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">)</span>
<span class="Comment"># Tensor[system.int] of shape &quot;[3, 8]&quot; on backend &quot;Cpu&quot;</span>
<span class="Comment"># |0      0     1     1     2     2     3     3|</span>
<span class="Comment"># |4      4     5     5     6     6     7     7|</span>
<span class="Comment"># |8      8     9     9    10    10    11    11|</span>

<span class="Identifier">echo</span> <span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">repeat_values</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="Identifier">axis</span> <span class="Operator">=</span> <span class="DecNumber">0</span><span class="Punctuation">)</span>
<span class="Comment"># Tensor[system.int] of shape &quot;[6, 4]&quot; on backend &quot;Cpu&quot;</span>
<span class="Comment"># |0      1     2     3|</span>
<span class="Comment"># |0      1     2     3|</span>
<span class="Comment"># |4      5     6     7|</span>
<span class="Comment"># |4      5     6     7|</span>
<span class="Comment"># |8      9    10    11|</span>
<span class="Comment"># |8      9    10    11|</span></pre></p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L575"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L575" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="repeat_values,Tensor[T],openArray[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#repeat_values%2CTensor%5BT%5D%2CopenArray%5Bint%5D"><span class="Identifier">repeat_values</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">reps</span><span class="Other">:</span> <span class="Identifier">openArray</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Create a new rank-1 tensor with each value <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span><span class="Punctuation">[</span><span class="Identifier">i</span><span class="Punctuation">]</span></span></tt> repeated <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span><span class="Punctuation">[</span><span class="Identifier">i</span><span class="Punctuation">]</span></span></tt> times</p>
<p>Compared to the version of <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat_values</span></span></tt> that takes a single integer <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> value this version always returns a rank-1 tensor (regardless of  and does not take the input shape) and does not take an axis argument.</p>
<p>Inputs:</p>
<ul class="simple"><li>t: A tensor.</li>
<li>reps: A sequence or array of integers indicating the number of times that each value must be repeated. It must have as many values as the input tensor.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A new rank-1 tensor containing the values of the input tensor repeated <tt class="docutils literal"><span class="pre"><span class="Identifier">reps</span></span></tt> times.</li>
</ul>
<p>Notes:</p>
<ul class="simple"><li>If a rep value is 0, the corresponding item in the input tensor will be skipped from the output.</li>
<li>The equivalent numpy function is called <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat</span></span></tt>, while the equivalent Matlab function is called <tt class="docutils literal"><span class="pre"><span class="Identifier">repelem</span></span></tt>. Different names where chosen here to avoid confusion with nim's <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat</span></span></tt> function which behaves like numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">tile</span></span></tt>, not like this function.</li>
</ul>
<p>Example:</p>
<p><pre class="listing">
<span class="Keyword">let</span> <span class="Identifier">t</span> <span class="Operator">=</span> <span class="Punctuation">[</span><span class="DecNumber">3</span><span class="Punctuation">,</span> <span class="DecNumber">5</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="DecNumber">4</span><span class="Punctuation">]</span><span class="Operator">.</span><span class="Identifier">toTensor</span>
<span class="Identifier">echo</span> <span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">repeat_values</span><span class="Punctuation">(</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">,</span> <span class="DecNumber">0</span><span class="Punctuation">,</span> <span class="DecNumber">3</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">]</span><span class="Punctuation">)</span>
<span class="Comment"># Tensor[system.int] of shape &quot;[6]&quot; on backend &quot;Cpu&quot;</span>
<span class="Comment">#     3     2     2     2     4     4</span></pre></p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L637"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L637" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="repeat_values,Tensor[T],Tensor[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#repeat_values%2CTensor%5BT%5D%2CTensor%5Bint%5D"><span class="Identifier">repeat_values</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">reps</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span><span class="Other">,</span>
    <span class="Identifier">inline</span>.}</pre></dt>
  <dd>
    
    <pre>Create a new rank-1 tensor with each value `t[i]` repeated `reps[i]` times

Overload of this function which takes a `Tensor[int]` instead of an
`openArray[int]`. Behavior is exactly the same as the `openArray[int]`
version.
```</pre>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L682"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L682" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="reshape-procs-all">
  <div id="reshape,Tensor,Metadata">
  <dt><pre><span class="Keyword">proc</span> <a href="#reshape%2CTensor%2CMetadata"><span class="Identifier">reshape</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">new_shape</span><span class="Other">:</span> <a href="datatypes.html#Metadata"><span class="Identifier">Metadata</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Reshape a tensor. If possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.</p>
<p>Input:</p>
<ul class="simple"><li>a tensor</li>
<li>a new shape. Number of elements must be the same</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with the same data but reshaped.</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L73"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L73" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="reshape,Tensor,varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#reshape%2CTensor%2Cvarargs%5Bint%5D"><span class="Identifier">reshape</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">new_shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Reshape a tensor. If possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.</p>
<p>Input:</p>
<ul class="simple"><li>a tensor</li>
<li>a new shape. Number of elements must be the same. Unlike numpy, dimensions cannot be -1 to infer their value. If that is what you need you must use the alternative <tt class="docutils literal"><span class="pre"><span class="Identifier">reshape_infer</span></span></tt> proc.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with the same data but reshaped.</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L59"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L59" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="reshape_infer-procs-all">
  <div id="reshape_infer,Tensor,varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#reshape_infer%2CTensor%2Cvarargs%5Bint%5D"><span class="Identifier">reshape_infer</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">new_shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Reshape a tensor. If possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.</p>
<p>Input:</p>
<ul class="simple"><li>a tensor</li>
<li>a new shape. Number of elements must be the same. The new shape can contain -1 to infer the size of one (and only one) dimension</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with the same data but reshaped.</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L85"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L85" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="roll-procs-all">
  <div id="roll,Tensor[T],int">
  <dt><pre><span class="Keyword">proc</span> <a href="#roll%2CTensor%5BT%5D%2Cint"><span class="Identifier">roll</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shift</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Roll elements of tensor &quot;globally&quot; (i.e. across all axes).</p>
<p>This takes a tensor, flattens it, rolls the elements <tt class="docutils literal"><span class="pre"><span class="Identifier">shift</span></span></tt> positions (taking the last <tt class="docutils literal"><span class="pre"><span class="Identifier">shift</span></span></tt> elements of the flattened tensor and putting them at the beginning of the flattened tensor), and then reshapes the rolled tensor back to the original shape.</p>
<p>This is different from the version of this proc that accepts an axis, which rolls _<a class="reference internal" href="#slices">slices</a> of a tensor taken along the selected axis.</p>
<p>Input:</p>
<ul class="simple"><li>t: Input tensor.</li>
<li>shift: Integer number of places by which elements are shifted.</li>
</ul>
<p>Return:</p>
<ul class="simple"><li>Output tensor, with the same shape as <tt class="docutils literal"><span class="pre"><span class="Identifier">a</span></span></tt>.</li>
</ul>
<p>Examples:<blockquote class="markdown-quote"><p>let x = arange(5) echo x.roll(2) Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#5">5</a>&quot; on backend &quot;Cpu&quot; 3     4     0     1     2 echo x.roll(-2) Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#5">5</a>&quot; on backend &quot;Cpu&quot; 2     3     4     0     1 let x2 = arange(5).reshape(2, 5) echo x2 # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |0 1 2 3 4| # |5 6 7 8 9| echo roll(x2, 1) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |9 0 1 2 3| # |4 5 6 7 8| echo roll(x2, -1) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |1 2 3 4 5| # |6 7 8 9 0|</p></blockquote>
</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L461"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L461" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="roll,Tensor[T],int,Natural">
  <dt><pre><span class="Keyword">proc</span> <a href="#roll%2CTensor%5BT%5D%2Cint%2CNatural"><span class="Identifier">roll</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shift</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    <p>Roll slices of a tensor along a given axis.</p>
<p>Slices that roll beyond the last position are re-introduced at the first.</p>
<p>Note that calling this proc with a rank-1 tensor, will simply check that <tt class="docutils literal"><span class="pre"><span class="Identifier">axis</span></span></tt> == 0 and then call the (axis-less) version of this proc.</p>
<p>Input:</p>
<ul class="simple"><li>t : Input tensor.</li>
<li>shift : Integer number of places by which elements are shifted.</li>
<li>axis : an axis (dimension).</li>
</ul>
<p>Return:</p>
<ul class="simple"><li>Output tensor, with the same shape as <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span></span></tt>.</li>
</ul>
<p>Notes:</p>
<ul class="simple"><li>numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">roll</span></span></tt> also supports passing a list of shifts and axis, while this proc doesn't. However, you can achieve the same effect by calling roll multiple times in a row (i.e. <tt class="docutils literal"><span class="pre"><span class="Identifier">np</span><span class="Operator">.</span><span class="Identifier">roll</span><span class="Punctuation">(</span><span class="Identifier">t</span><span class="Punctuation">,</span> <span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">]</span><span class="Punctuation">,</span> <span class="Identifier">axis</span><span class="Operator">=</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">,</span> <span class="DecNumber">1</span><span class="Punctuation">]</span><span class="Punctuation">)</span></span></tt> is equivalent to <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">roll</span><span class="Punctuation">(</span><span class="DecNumber">1</span><span class="Punctuation">,</span> <span class="Identifier">axis</span><span class="Operator">=</span><span class="DecNumber">0</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">roll</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="Identifier">axis</span><span class="Operator">=</span><span class="DecNumber">1</span><span class="Punctuation">)</span></span></tt> which is arguably more clear).</li>
</ul>
<p>Examples:<blockquote class="markdown-quote"><p>let x = arange(5) echo x.roll(2, axis=0) Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#5">5</a>&quot; on backend &quot;Cpu&quot; 3     4     0     1     2 echo x.roll(-2, axis=0) Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#5">5</a>&quot; on backend &quot;Cpu&quot; 2     3     4     0     1</p></blockquote>
</p>
<blockquote class="markdown-quote"><p>let x2 = arange(5).reshape(2, 5) echo x2 # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |0 1 2 3 4| # |5 6 7 8 9| echo roll(x2, 1, axis=0) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |5 6 7 8 9| # |0 1 2 3 4| echo roll(x2, -1, axis=0) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |5 6 7 8 9| # |0 1 2 3 4| echo roll(x2, 1, axis=1) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |4 0 1 2 3| # |9 5 6 7 8| echo roll(x2, -1, axis=1) # Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot; # |1 2 3 4 0| # |6 7 8 9 5| echo x2.roll(1, axis=0).roll(2, axis=1) Tensor<a class="reference internal" href="#system.int">system.int</a> of shape &quot;<a class="reference internal" href="#2, 5">2, 5</a>&quot; on backend &quot;Cpu&quot;|8 9 5 6 7| |3 4 0 1 2|</p></blockquote>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L501"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L501" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="split-procs-all">
  <div id="split,Tensor[T],Positive,Natural">
  <dt><pre><span class="Keyword">func</span> <a href="#split%2CTensor%5BT%5D%2CPositive%2CNatural"><span class="Identifier">split</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">chunk_size</span><span class="Other">:</span> <span class="Identifier">Positive</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> {.
    <span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Split the tensor into chunks of size <tt class="docutils literal"><span class="pre">chunk_size</span></tt> along the specified axis. Last chunk size will equal the remainder if the specified axis length is not divisible by <tt class="docutils literal"><span class="pre">chunk_size</span></tt>
    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L415"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L415" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="squeeze-procs-all">
  <div id="squeeze,AnyTensor">
  <dt><pre><span class="Keyword">func</span> <a href="#squeeze%2CAnyTensor"><span class="Identifier">squeeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#AnyTensor"><span class="Identifier">AnyTensor</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">AnyTensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Squeeze tensors. For example a Tensor of shape <a class="reference internal" href="#4,1,3">4,1,3</a> will become <a class="reference internal" href="#4,3">4,3</a> Input:<ul class="simple"><li>a tensor</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with singleton dimensions collapsed</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L375"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L375" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="squeeze,Tensor,Natural">
  <dt><pre><span class="Keyword">func</span> <a href="#squeeze%2CTensor%2CNatural"><span class="Identifier">squeeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Collapse the given axis, if the dimension is not 1, it does nothing. Input:<ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with that axis collapsed, if it was a singleton dimension</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L384"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L384" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="stack-procs-all">
  <div id="stack,varargs[Tensor[T]],Natural">
  <dt><pre><span class="Keyword">proc</span> <a href="#stack%2Cvarargs%5BTensor%5BT%5D%5D%2CNatural"><span class="Identifier">stack</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tensors</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span> <span class="Other">=</span> <span class="DecNumber">0</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> {.
    <span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Join a sequence of tensors along a new axis into a new tensor. Input:<ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a new stacked tensor along the new axis</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L405"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L405" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="tile-procs-all">
  <div id="tile,Tensor[T],varargs[int]">
  <dt><pre><span class="Keyword">proc</span> <a href="#tile%2CTensor%5BT%5D%2Cvarargs%5Bint%5D"><span class="Identifier">tile</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">reps</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
  <dd>
    
    <p>Construct a new tensor by repeating the input tensor a number of times on one or more axes</p>
<p>Inputs:</p>
<ul class="simple"><li>t: The tensor to repeat</li>
<li>reps: One or more integers indicating the number of times to repeat the tensor on each axis (starting with axis 0)</li>
</ul>
<p>Result:</p>
<ul class="simple"><li>A new tensor whose shape is <tt class="docutils literal"><span class="pre"><span class="Identifier">t</span><span class="Operator">.</span><span class="Identifier">shape</span> <span class="Operator">*.</span> <span class="Identifier">reps</span></span></tt></li>
</ul>
<p>Notes:</p>
<ul class="simple"><li>If a rep value is 1, the tensor is not repeated on that particular axis</li>
<li>If there are more rep values than the input tensor has axes, additional dimensions are prepended to the input tensor as needed. Note that this is similar to numpy's <tt class="docutils literal"><span class="pre"><span class="Identifier">tile</span></span></tt> function behavior, but different to Matlab's <tt class="docutils literal"><span class="pre"><span class="Identifier">repmat</span></span></tt> behavior, which appends missing dimensions instead of prepending them.</li>
<li>This function behavior is similar to nims <tt class="docutils literal"><span class="pre"><span class="Identifier">sequtils</span><span class="Operator">.</span><span class="Identifier">repeat</span></span></tt>, in that it repeats the full tensor multiple times. If what you want is to repeat the _<a class="reference internal" href="#elements">elements</a> of the tensor multiple times, rather than the full tensor, use the <tt class="docutils literal"><span class="pre"><span class="Identifier">repeat_values</span></span></tt> procedure instead.</li>
</ul>
<p>Examples:</p>
<p><pre class="listing">
<span class="Keyword">let</span> <span class="Identifier">x</span> <span class="Operator">=</span> <span class="Identifier">arange</span><span class="Punctuation">(</span><span class="DecNumber">4</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">reshape</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">)</span>

<span class="Comment"># When the number of reps and tensor dimensions match, the ouptut tensor</span>
<span class="Comment"># shape is the `reps *. t.shape`</span>
<span class="Identifier">echo</span> <span class="Identifier">tile</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="DecNumber">3</span><span class="Punctuation">)</span>
<span class="Operator">&gt;</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">system</span><span class="Operator">.</span><span class="Identifier">int</span><span class="Punctuation">]</span> <span class="Keyword">of</span> <span class="Identifier">shape</span> <span class="StringLit">&quot;[4, 6]&quot;</span> <span class="Identifier">on</span> <span class="Identifier">backend</span> <span class="StringLit">&quot;Cpu&quot;</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span><span class="Operator">|</span>

<span class="Comment"># If there are fewer reps than tensor dimensions, start</span>
<span class="Comment"># repeating on the first axis (leaving alone axis with missing reps)</span>
<span class="Identifier">echo</span> <span class="Identifier">tile</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">)</span>
<span class="Operator">&gt;</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">system</span><span class="Operator">.</span><span class="Identifier">int</span><span class="Punctuation">]</span> <span class="Keyword">of</span> <span class="Identifier">shape</span> <span class="StringLit">&quot;[4, 2]&quot;</span> <span class="Identifier">on</span> <span class="Identifier">backend</span> <span class="StringLit">&quot;Cpu&quot;</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span><span class="Operator">|</span>

<span class="Comment"># If there are more reps than tensor dimensions, prepend the missing</span>
<span class="Comment"># dimensions before repeating</span>
<span class="Identifier">echo</span> <span class="Identifier">tile</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">,</span> <span class="DecNumber">1</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="DecNumber">3</span><span class="Punctuation">)</span>
<span class="Operator">&gt;</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">system</span><span class="Operator">.</span><span class="Identifier">int</span><span class="Punctuation">]</span> <span class="Keyword">of</span> <span class="Identifier">shape</span> <span class="StringLit">&quot;[1, 4, 6]&quot;</span> <span class="Identifier">on</span> <span class="Identifier">backend</span> <span class="StringLit">&quot;Cpu&quot;</span>
<span class="Operator">&gt;</span>                 <span class="DecNumber">0</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">0</span>      <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span>     <span class="DecNumber">0</span>     <span class="DecNumber">1</span><span class="Operator">|</span>
<span class="Operator">&gt;</span> <span class="Operator">|</span><span class="DecNumber">2</span>      <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span>     <span class="DecNumber">2</span>     <span class="DecNumber">3</span><span class="Operator">|</span></pre></p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L691"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L691" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="transpose-procs-all">
  <div id="transpose,Tensor">
  <dt><pre><span class="Keyword">proc</span> <a href="#transpose%2CTensor"><span class="Identifier">transpose</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span><span class="Other">,</span> <span class="Identifier">inline</span>.}</pre></dt>
  <dd>
    
    <p>Transpose a Tensor.</p>
<p>For N-d Tensor with shape (0, 1, 2 ... n-1) the resulting tensor will have shape (n-1, ... 2, 1, 0)</p>
<p>Data is not copied or modified, only metadata is modified.</p>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L27"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L27" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>
<div id="unsqueeze-procs-all">
  <div id="unsqueeze,Tensor,Natural">
  <dt><pre><span class="Keyword">func</span> <a href="#unsqueeze%2CTensor%2CNatural"><span class="Identifier">unsqueeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> {.<span class="Identifier">noinit</span>.}</pre></dt>
  <dd>
    
    Insert a new axis just before the given axis, increasing the tensor dimension (rank) by 1 Input:<ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>a tensor with that new axis</li>
</ul>

    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L394"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L394" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>

  </dl>
</div>
<div class="section" id="18">
  <h1><a class="toc-backref" href="#18">Templates</a></h1>
  <dl class="item">
    <div id="bc-templates-all">
  <div id="bc.t,,Metadata">
  <dt><pre><span class="Keyword">template</span> <a href="#bc.t%2C%2CMetadata"><span class="Identifier">bc</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <span class="Other">(</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a> <span class="Operator">|</span> <span class="Identifier">SomeNumber</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="datatypes.html#Metadata"><span class="Identifier">Metadata</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">untyped</span></pre></dt>
  <dd>
    
    Alias for <tt class="docutils literal"><span class="pre">broadcast</span></tt>
    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L182"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L182" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>
<div id="bc.t,,varargs[int]">
  <dt><pre><span class="Keyword">template</span> <a href="#bc.t%2C%2Cvarargs%5Bint%5D"><span class="Identifier">bc</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <span class="Other">(</span><a href="datatypes.html#Tensor"><span class="Identifier">Tensor</span></a> <span class="Operator">|</span> <span class="Identifier">SomeNumber</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">untyped</span></pre></dt>
  <dd>
    
    Alias for <tt class="docutils literal"><span class="pre">broadcast</span></tt>
    &nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/arraymancer/tensor/shapeshifting.nim#L178"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/arraymancer/tensor/shapeshifting.nim#L178" class="link-seesrc" target="_blank" >Edit</a>

  </dd>
</div>

</div>

  </dl>
</div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small style="color: var(--hint);">Made with Nim. Generated: 2024-05-12 14:00:11 UTC</small>
      </div>
    </div>
  </div>
</div>

<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace" style="padding-bottom: 15px; padding-top: 10px;">
      <span>
      <li>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="accessors.html">accessors</a></li>
<li><a href="accessors_macros_read.html">accessors_macros_read</a></li>
<li><a href="accessors_macros_syntax.html">accessors_macros_syntax</a></li>
<li><a href="accessors_macros_write.html">accessors_macros_write</a></li>
<li><a href="aggregate.html">aggregate</a></li>
<li><a href="algorithms.html">algorithms</a></li>
<li><a href="blas_l3_gemm.html">blas_l3_gemm</a></li>
<li><a href="complex.html">complex</a></li>
<li><a href="cublas.html">cublas</a></li>
<li><a href="cuda.html">cuda</a></li>
<li><a href="cuda_global_state.html">cuda_global_state</a></li>
<li><a href="data_structure.html">data_structure</a></li>
<li><a href="display.html">display</a></li>
<li><a href="display_cuda.html">display_cuda</a></li>
<li><a href="einsum.html">einsum</a></li>
<li><a href="exporting.html">exporting</a></li>
<li><a href="filling_data.html">filling_data</a></li>
<li><a href="higher_order_applymap.html">higher_order_applymap</a></li>
<li><a href="higher_order_foldreduce.html">higher_order_foldreduce</a></li>
<li><a href="incl_accessors_cuda.html">incl_accessors_cuda</a></li>
<li><a href="incl_higher_order_cuda.html">incl_higher_order_cuda</a></li>
<li><a href="incl_kernels_cuda.html">incl_kernels_cuda</a></li>
<li><a href="init_copy_cpu.html">init_copy_cpu</a></li>
<li><a href="init_copy_cuda.html">init_copy_cuda</a></li>
<li><a href="init_cpu.html">init_cpu</a></li>
<li><a href="init_cuda.html">init_cuda</a></li>
<li><a href="init_opencl.html">init_opencl</a></li>
<li><a href="lapack.html">lapack</a></li>
<li><a href="math_functions.html">math_functions</a></li>
<li><a href="memory_optimization_hints.html">memory_optimization_hints</a></li>
<li><a href="naive_l2_gemv.html">naive_l2_gemv</a></li>
<li><a href="opencl_backend.html">opencl_backend</a></li>
<li><a href="opencl_global_state.html">opencl_global_state</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="operators_blas_l1.html">operators_blas_l1</a></li>
<li><a href="operators_blas_l1_cuda.html">operators_blas_l1_cuda</a></li>
<li><a href="operators_blas_l1_opencl.html">operators_blas_l1_opencl</a></li>
<li><a href="operators_blas_l2l3.html">operators_blas_l2l3</a></li>
<li><a href="operators_blas_l2l3_cuda.html">operators_blas_l2l3_cuda</a></li>
<li><a href="operators_blas_l2l3_opencl.html">operators_blas_l2l3_opencl</a></li>
<li><a href="operators_broadcasted.html">operators_broadcasted</a></li>
<li><a href="operators_broadcasted_cuda.html">operators_broadcasted_cuda</a></li>
<li><a href="operators_broadcasted_opencl.html">operators_broadcasted_opencl</a></li>
<li><a href="operators_comparison.html">operators_comparison</a></li>
<li><a href="operators_logical.html">operators_logical</a></li>
<li><a href="optim_ops_fusion.html">optim_ops_fusion</a></li>
<li><a href="p_accessors.html">p_accessors</a></li>
<li><a href="p_accessors_macros_desugar.html">p_accessors_macros_desugar</a></li>
<li><a href="p_accessors_macros_read.html">p_accessors_macros_read</a></li>
<li><a href="p_accessors_macros_write.html">p_accessors_macros_write</a></li>
<li><a href="p_checks.html">p_checks</a></li>
<li><a href="p_complex.html">p_complex</a></li>
<li><a href="p_display.html">p_display</a></li>
<li><a href="p_empty_tensors.html">p_empty_tensors</a></li>
<li><a href="p_init_cuda.html">p_init_cuda</a></li>
<li><a href="p_init_opencl.html">p_init_opencl</a></li>
<li><a href="p_kernels_interface_cuda.html">p_kernels_interface_cuda</a></li>
<li><a href="p_kernels_interface_opencl.html">p_kernels_interface_opencl</a></li>
<li><a href="p_operator_blas_l2l3.html">p_operator_blas_l2l3</a></li>
<li><a href="p_shapeshifting.html">p_shapeshifting</a></li>
<li><a href="selectors.html">selectors</a></li>
<li><a href="shapeshifting.html">shapeshifting</a></li>
<li><a href="shapeshifting_cuda.html">shapeshifting_cuda</a></li>
<li><a href="shapeshifting_opencl.html">shapeshifting_opencl</a></li>
<li><a href="syntactic_sugar.html">syntactic_sugar</a></li>
<li><a href="tensor_cuda.html">tensor_cuda</a></li>
<li><a href="tensor_opencl.html">tensor_opencl</a></li>
<li><a href="ufunc.html">ufunc</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="conv2D.html">Layers: Convolution 2D</a></li>
<li><a href="cross_entropy_losses.html">Loss: Cross-Entropy losses</a></li>
<li><a href="embedding.html">Layers: Embedding</a></li>
<li><a href="flatten.html">flatten</a></li>
<li><a href="gcn.html">gcn</a></li>
<li><a href="gru.html">Layers: GRU (Gated Linear Unit)</a></li>
<li><a href="init.html">Layers: Initializations</a></li>
<li><a href="linear.html">Layers: Linear/Dense</a></li>
<li><a href="maxpool2D.html">Layers: Maxpool 2D</a></li>
<li><a href="mean_square_error_loss.html">Loss: Mean Square Error</a></li>
<li><a href="nn_dsl.html">Neural network: Declaration</a></li>
<li><a href="optimizers.html">Optimizers</a></li>
<li><a href="relu.html">Activation: Relu (Rectified linear Unit)</a></li>
<li><a href="sigmoid.html">Activation: Sigmoid</a></li>
<li><a href="softmax.html">Softmax</a></li>
<li><a href="tanh.html">Activation: Tanh</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">Linear algebra, stats, ML</a>
        <ul class="monospace">
          <li><a href="accuracy_score.html">Accuracy score</a></li>
<li><a href="algebra.html">algebra</a></li>
<li><a href="auxiliary_blas.html">auxiliary_blas</a></li>
<li><a href="auxiliary_lapack.html">auxiliary_lapack</a></li>
<li><a href="common_error_functions.html">Common errors, MAE and MSE (L1, L2 loss)</a></li>
<li><a href="dbscan.html">dbscan</a></li>
<li><a href="decomposition.html">Eigenvalue decomposition</a></li>
<li><a href="decomposition_lapack.html">decomposition_lapack</a></li>
<li><a href="decomposition_rand.html">Randomized Truncated SVD</a></li>
<li><a href="distributions.html">distributions</a></li>
<li><a href="init_colmajor.html">init_colmajor</a></li>
<li><a href="kde.html">kde</a></li>
<li><a href="kmeans.html">K-Means</a></li>
<li><a href="least_squares.html">Least squares solver</a></li>
<li><a href="least_squares_lapack.html">least_squares_lapack</a></li>
<li><a href="linear_systems.html">Linear systems solver</a></li>
<li><a href="overload.html">overload</a></li>
<li><a href="pca.html">Principal Component Analysis (PCA)</a></li>
<li><a href="solve_lapack.html">solve_lapack</a></li>
<li><a href="special_matrices.html">Special linear algebra matrices</a></li>
<li><a href="stats.html">Statistics</a></li>
<li><a href="triangular.html">triangular</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">IO & Datasets</a>
        <ul class="monospace">
          <li><a href="imdb.html">IMDB</a></li>
<li><a href="io_csv.html">CSV reading and writing</a></li>
<li><a href="io_hdf5.html">HDF5 files reading and writing</a></li>
<li><a href="io_image.html">Images reading and writing</a></li>
<li><a href="io_npy.html">Numpy files reading and writing</a></li>
<li><a href="io_stream_readers.html">io_stream_readers</a></li>
<li><a href="mnist.html">MNIST</a></li>
<li><a href="util.html">util</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="autograd_common.html">Data structure</a></li>
<li><a href="gates_basic.html">Basic operations</a></li>
<li><a href="gates_blas.html">Linear algebra operations</a></li>
<li><a href="gates_hadamard.html">Hadamard product (elementwise matrix multiply)</a></li>
<li><a href="gates_reduce.html">Reduction operations</a></li>
<li><a href="gates_shapeshifting_concat_split.html">Concatenation, stacking, splitting, chunking operations</a></li>
<li><a href="gates_shapeshifting_views.html">Linear algebra operations</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="conv.html">conv</a></li>
<li><a href="cudnn.html">cudnn</a></li>
<li><a href="cudnn_conv_interface.html">cudnn_conv_interface</a></li>
<li><a href="nnp_activation.html">Activations</a></li>
<li><a href="nnp_conv2d_cudnn.html">Convolution 2D - CuDNN</a></li>
<li><a href="nnp_convolution.html">Convolution 2D</a></li>
<li><a href="nnp_embedding.html">Embeddings</a></li>
<li><a href="nnp_gru.html">Gated Recurrent Unit (GRU)</a></li>
<li><a href="nnp_linear.html">Linear / Dense layer</a></li>
<li><a href="nnp_maxpooling.html">Maxpooling</a></li>
<li><a href="nnp_numerical_gradient.html">Numerical gradient</a></li>
<li><a href="nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
<li><a href="nnp_softmax.html">Softmax</a></li>
<li><a href="nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
<li><a href="nnpack.html">nnpack</a></li>
<li><a href="nnpack_interface.html">nnpack_interface</a></li>
<li><a href="p_activation.html">p_activation</a></li>
<li><a href="p_logsumexp.html">p_logsumexp</a></li>
<li><a href="p_nnp_checks.html">p_nnp_checks</a></li>
<li><a href="p_nnp_types.html">p_nnp_types</a></li>
        </ul>
      </li>
      </span>
      <span>
      <li>
        <a href="#">Other docs</a>
        <ul class="monospace">
          <li><a href="align_unroller.html">align_unroller</a></li>
<li><a href="ast_utils.html">ast_utils</a></li>
<li><a href="compiler_optim_hints.html">compiler_optim_hints</a></li>
<li><a href="cpuinfo_x86.html">cpuinfo_x86</a></li>
<li><a href="datatypes.html">datatypes</a></li>
<li><a href="deprecate.html">deprecate</a></li>
<li><a href="dynamic_stack_arrays.html">dynamic_stack_arrays</a></li>
<li><a href="foreach.html">foreach</a></li>
<li><a href="foreach_common.html">foreach_common</a></li>
<li><a href="foreach_staged.html">foreach_staged</a></li>
<li><a href="functional.html">functional</a></li>
<li><a href="gemm.html">gemm</a></li>
<li><a href="gemm_packing.html">gemm_packing</a></li>
<li><a href="gemm_prepacked.html">gemm_prepacked</a></li>
<li><a href="gemm_tiling.html">gemm_tiling</a></li>
<li><a href="gemm_ukernel_avx.html">gemm_ukernel_avx</a></li>
<li><a href="gemm_ukernel_avx2.html">gemm_ukernel_avx2</a></li>
<li><a href="gemm_ukernel_avx512.html">gemm_ukernel_avx512</a></li>
<li><a href="gemm_ukernel_avx_fma.html">gemm_ukernel_avx_fma</a></li>
<li><a href="gemm_ukernel_dispatch.html">gemm_ukernel_dispatch</a></li>
<li><a href="gemm_ukernel_generator.html">gemm_ukernel_generator</a></li>
<li><a href="gemm_ukernel_generic.html">gemm_ukernel_generic</a></li>
<li><a href="gemm_ukernel_sse.html">gemm_ukernel_sse</a></li>
<li><a href="gemm_ukernel_sse2.html">gemm_ukernel_sse2</a></li>
<li><a href="gemm_ukernel_sse4_1.html">gemm_ukernel_sse4_1</a></li>
<li><a href="gemm_utils.html">gemm_utils</a></li>
<li><a href="global_config.html">global_config</a></li>
<li><a href="initialization.html">initialization</a></li>
<li><a href="math_ops_fusion.html">math_ops_fusion</a></li>
<li><a href="memory.html">memory</a></li>
<li><a href="nested_containers.html">nested_containers</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="sequninit.html">sequninit</a></li>
<li><a href="simd.html">simd</a></li>
<li><a href="tokenizers.html">tokenizers</a></li>
        </ul>
      </li>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
      <li><a href="uth.opencl_cuda_nim.html">Working with OpenCL and Cuda in Nim</a></li>
    </ul>
  </span>
</header>
</body>
</html>
